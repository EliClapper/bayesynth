---
title: "Report"
author: "Caspar van Lissa & Eli Clapper"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

# Intro
Report on the results

<!--
call library and load in results. The `dat` `data.table` is subsetted for all cases where $es >= .1$ . This removes 108/3 = 36 conditions from the results.
-->

```{r, echo = F}
library(data.table) # load in libraries
library(DT)
library(ggplot2)
rename <- c("prodbf" = "PBF", "gpbf" = "gPBF", "tbf" = "TBF", "ipd" = "IPD", "rma" = "RMA", "allsig" = "VC")

```

# Results

For each algorithm, a confusion matrix was obtained by tabulating inferential decisions made using that algorithm against the population status of the hypothesis (true or false).
For all BFs, the informative hypothesis was accepted if BF > 3.
For the IPD and RMA meta-analysis, the null-hypothesis was rejected if the confidence interval for the overall effect size excluded the hypothesized value.
Finally, for the vote count (VC) algorithm, the null was rejected if all individual group hypotheses were rejected.
Table \@ref(tab:tabconf) shows the resulting confusion matrix metrics.
The only algorithm that shows relatively high values for both sensitivity and specificity is the product Bayes factor (PBF).
This is also reflected in the high accuracy, which represents the total proportion of correct (true positive/true negative) decisions.

```{r tabconf, echo = F, message=F}
tab <- read.csv("../confusion.csv", stringsAsFactors = FALSE)
names(tab)[1] <- "Metric"
tab$Metric <- gsub("^.+_", "", tab$Metric)
tab <- tab[!tab$Metric == "lr", !grepl("iu", names(tab))]
names(tab) <- gsub("_ic", "", names(tab))
# names(tab)[match(names(rename), names(tab))] <- rename
tab <- tab[tab$Metric %in% c("sensitivity", "specificity", "accuracy"),]
rownames(tab) <- NULL
knitr::kable(tab, digits = 2, caption = "Marginal confusion matrix metrics.")
```
These results indicate that all algorithms except PBF had low sensitivity to detect a true effect.
In contrast, specificity was very high for all algorithms except PBF.
This suggests that the other algorithms classified most conditions as negatives (no effect found),
regardless of the existence of a population effect.
The PBF trades a loss of specificity for increased sensitivity,
and average levels of both were approximately equal.
As the PBF vastly outperformed other BFs, these were omitted from further analysis.

## Effect of simulation conditions

We examined the effect of simulation conditions on overall accuracy.

```{r tabeffect, echo = FALSE}
tab <- read.csv("../effect_of_conditions.csv", stringsAsFactors = FALSE)
tab <- tab[, !grepl("(gpbf|tbf)", names(tab))]
tab <- tab[, grepl("^\\w+$", names(tab)) | (grepl("\\.{2}", names(tab)) & grepl("PBF", names(tab)))]
names(tab)[grepl("\\.{2}", names(tab))] <- paste0("vs ", gsub("(\\.|PBF|vs)", "", names(tab)[grepl("\\.{2}", names(tab))]))
knitr::kable(tab, digits = 2, caption = "Partial eta squared of the effect of each design factor on accuracy for each algorithm and for the difference between PBF and all other algorithms (e.g., vs RMA).")
```
PBF performance was most impacted by sample size $n$, followed by the number of groups $k$, and residual heterogeneity.

### Effect of sample size

```{r fign, fig.cap="Mean performance by sample size"}
res <- readRDS("../confusion_by_cond.RData")
res <- res[res$alg %in% c("IPD", "RMA", "VC", "PBF"), ]

df_plot <- melt(res, measure.vars = c("sensitivity", "specificity"),
             variable.name = "Outcome", value.name = "val")
df_plot <- df_plot[ ,list(mean=mean(val)), by=c("n", "Outcome", "alg")]
ggplot(df_plot, aes(x = n, y = mean, group = interaction(Outcome, alg), shape = alg, linetype = Outcome)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(y = "Mean performance", x = "Sample size")
```

Figure \@ref(fig:fign) indicates that for PBF, both sensitivity and specificity increase with sample size.
The other algorithms also show increasing sensitivity, but not specificity, which is at a ceiling.
This difference explains the effect of errorsd on the diference between algorithms (see Table \@ref(tab:tabeffect)).

### Effect of number of groups

```{r, fig.cap="Mean performance by number of groups"}
df_plot <- melt(res, measure.vars = c("sensitivity", "specificity"),
             variable.name = "Outcome", value.name = "val")
df_plot <- df_plot[ ,list(mean=mean(val)), by=c("k", "Outcome", "alg")]
ggplot(df_plot, aes(x = k, y = mean, group = interaction(Outcome, alg), shape = alg, linetype = Outcome)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(y = "Mean performance", x = "Number of groups")
```
The figure indicates that, for PBF at higher levels of k, lower sensitivity is exchanged for greater specificity.
The other algorithms do not show this pattern, as their specificity is at a ceiling.
This difference in pattern of effects explains why number of groups has a moderate effect on the difference between algorithms (see Table \@ref(tab:tabeffect)).
Only VC shows decreasing sensitivity with an increasing number of groups;
this is because the probability of obtaining any false negatives increases with the number of groups.


### Effect of residual heterogeneity

```{r, fig.cap="Mean performance by errorsd"}
df_plot <- melt(res, measure.vars = c("sensitivity", "specificity"),
             variable.name = "Outcome", value.name = "val")
df_plot <- df_plot[ ,list(mean=mean(val)), by=c("reliability", "Outcome", "alg")]
ggplot(df_plot, aes(x = reliability, y = mean, group = interaction(Outcome, alg), shape = alg, linetype = Outcome)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(y = "Mean performance")
```

The figure indicates that for PBF, at higher levels of residual heterogeneity, lower sensitivity is exchanged for greater specificity.
The other algorithms do not show this pattern, as their specificity is at a ceiling.
Their sensitivity decreases with residual heterogeneity,
and therefore, so does their overall performance.
This difference in pattern of effects explains why residual heterogeneity has a moderate effect on the difference between algorithms (see Table \@ref(tab:tabeffect)).
Note that, in contrast to other algorithms, the performance of PBF is less susceptible to residual heterogeneity.


# Discussion

Within the scope of this simulation, PBF had relatively better inferential properties than the other algorithms under consideration.
Although other algorithms had superior specificity, PBF had the highest levels of sensitivity.
Thus, PBF balanced the ability to accept an informative hypothesis in the presence of a true population effect with the ability to reject the hypothesis when there was no effect.
<!-- Conventional threshold for type 1 error: 5% -->
<!-- Conventional threshold for power: 80% -->
<!-- J. Cohen, Statistical Power Analysis for the -->
<!--  Behavioral Sciences, 2nd ed. (Erlbaum, Hillsdale, -->
<!--  NJ, 1988). This is the source of the system of power -->
<!--  analysis described here; the power values and sam -->
<!--  ple sizes of the illustrations derive from this book's -->
<!--  tables. -->
<!--  2. J -->

An important caveat is that none of the algorithms met conventional criteria for power (80%) and type I error (5%).
Overall, PBF had the best performance, with power of 76% and type I error of 24%.
Thus, if researchers intend to synthesize evidence for an informative hypothesis across heterogeneous studies, PBF may be the most suitable method - but its limitations should be acknowledged in applied research.

The overall performance of PBF increased most with increasing sample size.
With an increasing number of groups, slightly decreasing specificity was traded off for increasing sensitivity.
With increasing residual heterogeneity, increasing specificity was traded off for decreasing sensitivity.

<!-- # Conclusions -->

<!-- All in all it can be said the the algorithms are conservative. In almost all conditions for all algorithms, the Specificity was higher than the Sensitivity. In fact, the only condition and algorithm when this was reversed was for `prodbf_ic` when $k = 10$ and $errorsd = 0$. This conservatism can be interpreted as good, because the probability of making a Type I error is low. However, the finding that the Sensitivity was rather low implies that there is almost no power to detect an effect. The current study only simulated conditions when the true effect size was .1 higher than the hypothesized value. Although, this is closer to reality as effect sizes in populations are rarely large. In smaller samples, larger effect sizes are more plausible however and its interesting to see if this conservatism is retained by the algorithms as the differences between $es$ and $hyp_val$ gets larger -->

<!-- There were some exceptions that seemed more promising. These only apply to the `prodbf_ic` and `tbf_ic` algorithms, however. These conditions were mainly if both $k$ and $n$ are large. Real life examples usually do not have data of 10 groups, each containing 500 observations, although within the Baysian Framework this is more plausible with the use of historical datasets in current analyses. -->

<!-- The variation in conditions mainly affected `prodbf_ic`. The initial guess is that `prodbf_ic` can be unstable as it takes the product of the BFs for all $k$ groups. Suppose $k = 10$ and for all is found that $BF_{ic} = 2$, the `prodbf_ic` will output $2^{10} = 1024$, although no group on its own is convincingly in favour of any hypotheses. The `tbf` is more stable as it evaluates if all BFs together exceed 3. The `gpbf` is the `prodbf`$^{\frac{1}{k}}$ which for our particular example makes $2^{10*\frac{1}{10}} = 2^1 = 2$. This is the main reason that as $errorsd = 0$, the `prodbf_ic` gets more extreme as the probability of consensus between groups becomes larger when $es > hyp\_val$. -->

<!-- The reason that the `iu` algorithms are more conservative is because they are not constrained by testing against only the complement of the specified hypothesis. -->

<!-- The overall conclusion is that the algorithms lack power to detect an effect and can not realistically be used. Only if much data is available on multiple groups do the `prodbf_ic` and `tbf_ic` become useful, Where the `prodbf_ic` sacrifices a bit more Specificity than `tbf_ic` for more Sensitivity.  -->


<!-- ```{r, echo = F, eval = F} -->
<!-- rm(list = ls()) -->
<!-- gc(full = T, reset = T) -->
<!-- cat("\014") -->
<!-- ``` -->

