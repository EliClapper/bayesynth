---
title: "Report"
author: "Eli Clapper"
date: "23/03/2022"
output: html_document
---

# Intro
Report on the results

call library and load in results
```{r, echo = F}
library(data.table) # load in libraries
library(DT)
dat <- readRDS(file.path("Sim_Eli", "sim_results_2022-03-19.RData")) #
```

some preparatory variables
```{r, echo = F}
varsout <- c("allsig", "gpbf_ic", "gpbf_iu", "prodbf_ic", "prodbf_iu", "tbf_ic", "tbf_iu")
varspred <- c("es", "errorsd", "n", "k", "hyp_val")
```


## Marginal Descriptives
Obtain marginal descriptives
```{r, echo = F}
descriptives <- function(datf){ 
  descs <- as.matrix(
    datf[, lapply(.SD, function(var){
    medians <- median(var) # obtain overall medians per algorithm over medians per condition
    mads <- mad(var)       # obtain Median absolute deviations per conditions
    sums <- sum(var>3)     # obtain sums where the median is > 3
    maxvals <- max(var)    # obtain the highest median
    c(medians, mads, sums, maxvals)
  }), .SDcols = varsout])
  rownames(descs) <- c("median", "mad", "sum>3", "max")
  descs
}
marginal_descs <- descriptives(dat[ ,lapply(.SD, median), .SDcols = varsout, by = varspred])
datatable(round(marginal_descs,5), options = list(dom = 't'))

```
In theory, the algorithms should have 108/3 = 36 conditions where the median BF > 3. This is because BF > 3 only
in the conditions where true_es > hyp_val, which is one third of the time.
The marginal median for all algorithms is < 1, suggesting that the BF in most conditions is indeed < 3.
The Median Absolute Deviations are also not too severe, giving more proof for reliable estimates.
Interestingly, the prodbf_ic does have a notable higher mad, which is likely due to its outliers.
The finding that the algorithms are rather conservative is further supported by the number of times
the median BF does exceed 3. Both gpbf algorithms count 0 conditions where median BF > 3, while for allsig this
number is 1. For the prodbf_iu and tbf algorithms 5 or 6 times is counted and only the prodbf seems to have a 
more notable number of times the median BF > 3, namely 16.

Now subset for when true BF > 3
```{r, echo = F}
es_gt_hyp_val <- descriptives(dat[es == .2 ,lapply(.SD, median), .SDcols = varsout, by = varspred])
datatable(round(es_gt_hyp_val,5),options = list(dom = "t"))
```
```{r}
all(es_gt_hyp_val['sum>3',] == marginal_descs['sum>3',]) 
```

Even when we subset for when es > hyp_val no median gets above 3. Also, all cases where the median BF > 3
is indeed when es > hyp_val. Again, the MADs are not too severe, except for the prodbf_ic, suggesting the algorithm is less stable when BF > 3.

When true BF < 3
```{r, echo =F}
es_st_hyp_val <- descriptives(dat[es < .2 ,lapply(.SD, median), .SDcols = varsout, by = varspred])
datatable(round(es_st_hyp_val,5), options = list(dom = 't'))
```
Also interesting is that the maximum median BF in the cases where es <= hyp_val all are ~1. 
this provides more support for conservative algorithms. Because the BFs has an interval of 0-$\infty$, it is expected that the mad for the cases where BF < 3 is especially low. But again, the MAD of the prodbf_ic is higher than its median, suggesting less stability even when BF < 3.


## Confusion matrix statistics
```{r, echo = F}
# obtain confusion matrix metrics
lapply_at <- function(var, truees) {
  results <- sapply(var, function(var) {
    table(ordered(truees > .1, levels = c("FALSE", "TRUE")), ordered(var > 3, levels = c("FALSE", "TRUE")))
  })
  names(results) <- vapply(names(var), paste, c("TN", "FN", "FP", "TP"), sep = "_", 
                           FUN.VALUE = character(4),
                           USE.NAMES = FALSE)
  as.list(results)
}

# in long format
#ORIGINALLY HAD datf[!es < .1], but I do not know why.
metrics <- function(datf){
  res <- datf[, sapply(.SD, function(var) {
    table(ordered(es > .1, levels = c("FALSE", "TRUE")), ordered(var > 3, levels = c("FALSE", "TRUE")))
  }), .SDcols = varsout]
  rownames(res) <- c("TN", "FN", "FP", "TP")
  return(res)
}

# obtain metrics per algorithm()
get_stats <- function(res){ 
  apply(res, 2, function(x){
    attach(as.list(x))
    alpha = FP / (FP + TN) #False positive rate 
    beta = FN / (TP + FN) # False negative rate
    sensitivity = 1-beta
    specificity = 1-alpha
    correct = (TN + TP) /(FP + FN + TN + TP)
    pos_lr = sensitivity / (1 - specificity)
    neg_lr = (1 - sensitivity) / specificity
    c(correct = correct,
      False_positive_rate_alpha = alpha,
      False_negative_rate_beta = beta,
      sensitivity = sensitivity,
      specificity = specificity, 
      pos_lr = ifelse(is.na(pos_lr), 0, pos_lr),
      neg_lr = ifelse(is.na(neg_lr), 0, neg_lr))
  })
}


library(ggplot2)
plot_df <-function(stats){ 
  data.frame(Method = colnames(stats),
        correct = stats["correct", ],
        Sensitivity = stats["sensitivity", ],
        Specificity = stats["specificity", ],
        FPR = stats["False_positive_rate_alpha", ],
        FNR = stats["False_negative_rate_beta", ],
        Pos_lr = stats["pos_lr", ],
        Neg_lr = stats["neg_lr", ])
}

#create plot df for subset of data
create_df_plot <- function(datf){
  metrics_long <- metrics(datf)
  stats <- get_stats(metrics_long)
  df_plot <- plot_df(stats)
  return(df_plot)
}
```

Marginal statistics
```{r, echo = F, message=F}
df_plot <- create_df_plot(dat)
marginal_metrics <- rbind(df_plot[,2:ncol(df_plot)], 
      mean = apply(df_plot[,2:ncol(df_plot)], 2 ,mean, na.rm = T))
datatable(round(marginal_metrics, 5), options = list(dom = 't'))
```
Firstly, on average, the algorithms were correct in 72\% of the cases. The prodbf_ic did best with 79\% identified correctly, closely followed by 74\% of the tbf_ic.The gbpf algorithms do overall worst with 67\% being identified correctly. Also interestingly, all _ic algorithms do better than their _iu counterparts.
Secondly, all measures suggest somewhat conservative algorithms, with a mild exception for prodbf_ic. The mean sensitivity of the algorithms is 0.17, meaning that on average, out of all cases where BF > 3, only 17\% is identified correctly. Interestingly, the gpbf algorithms are especially conservative, with the gpbf_iu not having a single case where its BF > 3. The tbf algorithms and prodbf_iu do slightly better in this regard. An obvious outlier is the prodbf_ic algorithm with 44\% correctly identified when BF > 3.
Where the algorithms make up is in the specificity, with a mean specificty of 0.98. This implies that out of all cases where BF < 3, 98\% is identified correctly on average. The prodbf_ic performs only slightly worse than the other algorithms, although the differences are close to minimal.

<!--add part about LR+ and LR- -->


<!-- below is in the make...
```{r, eval = F}
# conditional on n
ndf <- rbind(create_df_plot(dat[n==20]),
           create_df_plot(dat[n==80]),
           create_df_plot(dat[n==200]),
           create_df_plot(dat[n==500]))
ndf$cond <- c(rep("n = 20", 7), rep("n = 80", 7), rep("n = 200", 7), rep("n = 500", 7))

ggplot(ndf, aes(x = Sensitivity, y = Specificity, colour = Method, shape = cond)) +
  geom_point() +
  scale_x_continuous(limits = c(0,1))+
  scale_y_continuous(limits = c(0.75,1)) +
  theme_bw()


# conditional on errorsd
errorsddf <- rbind(create_df_plot(dat[errorsd == 0]),
             create_df_plot(dat[errorsd == 0.5]),
             create_df_plot(dat[errorsd == 0.81]))
errorsddf$cond <- c(rep("errorsd = 0", 7), rep("errorsd = 0.5", 7), rep("errorsd = 0.81", 7))

ggplot(errorsddf, aes(x = Sensitivity, y = Specificity, colour = Method, shape = cond)) +
  geom_point() +
  scale_x_continuous(limits = c(0,1))+
  scale_y_continuous(limits = c(0,1)) +
  theme_bw()

```
-->
