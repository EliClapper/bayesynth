---
title: "Report"
author: "Eli Clapper"
date: "23/03/2022"
output: html_document
---

# Intro
Report on the results

<!--
call library and load in results. The `dat` `data.table` is subsetted for all cases where $es \geq .1$ . This effectively removes 108/3 = 36 conditions from the results.
-->
```{r, echo = F}
library(data.table) # load in libraries
library(DT)
library(ggplot2)
dat <- readRDS(file.path("Sim_Eli", "sim_results_2022-03-19.RData")) #
dat <- dat[!es < .1]
```

<!-- Some preparatory variables which are the algorithms (varsout) and the conditions (varspred) -->
```{r, echo = F}
varsout <- c("allsig", "gpbf_ic", "gpbf_iu", "prodbf_ic", "prodbf_iu", "tbf_ic", "tbf_iu")
varspred <- c("es", "errorsd", "n", "k", "hyp_val")
```


## Marginal Descriptives
<!--
Obtain marginal descriptives for all algorithms separately. This function first aggregates all conditions by taking the median over all 1000 simulation runs on that condition, reducing the `data.table` to 72000/1000 = 72 medians for the different conditions. Finally it describes the distribution of medians of these 72 conditions for every algorithm. It takes some quantiles, the mean absolute deviations, the sum of where BF > 3 and the range. Later on, decision theory metrics such as Sensitivity and Positive Likelihood Ratio are calculated on the non-aggregated `data.table`.
-->
```{r, echo = F}
descriptives <- function(datf){ 
  descs <- as.matrix(
    datf[, lapply(.SD, function(var){
    quants <- quantile(var, c(0.25, 0.50, 0.75, 0.90, 0.95))# obtain overall quantiles
    mads <- mad(var)       # obtain Median absolute deviations per conditions
    sums <- sum(var>3)     # obtain sums where the median is > 3
    maxvals <- max(var)    # obtain the highest median
    minvals <- min(var)    # obtain lowest median
    c(quants[1], quants[2], quants[3], quants[4], quants[5], mads, sums, maxvals, minvals)
  }), .SDcols = varsout])
  rownames(descs) <- c('25 %', '50 %',  '75 %','90 %', '95 %', "mad", "sum>3", "max", "min")
  descs
}
marginal_descs <- descriptives(dat[ ,lapply(.SD, median), .SDcols = varsout, by = varspred])
datatable(round(marginal_descs,5), options = list(dom = 't'))
```
Optimally, the algorithms should have $\frac{72}{2} = 36$ conditions where the median BF > 3.
The marginal median for all algorithms is $\approx 1.0$, however. Even at the $90^{th}$ percentile only the prodbf_ic algorithm has its median BF > 3. The Median Absolute Deviations are not too severe, providing evidence for reliable estimates. Prodbf_ic does have a notably higher mad, which is likely due to its large valued outliers.
The finding that the algorithms are rather conservative is further supported by the number of times
the median BF does exceed 3. Both gpbf algorithms count 0 conditions where median BF > 3, while for allsig this
number is 1. For the prodbf_iu and tbf algorithms 5 or 6 times is counted and only the prodbf seems to have a 
more notable number of times the median BF > 3, namely 16. 
Another observation is that the informative BFs against the unconstrained hypotheses seem to be a bit more conservative than their complement hypotheses counterparts. 
Finally, it the columns for `prodbf_iu` and `tbf_iu` are exactly equal. It might be that these algorithms do exactly the same thing or there was an error in the simulation. 

Let us check it out
```{r, echo=F}
cor(dat[,..varsout])
```
`tbf_iu` and `prodbf_iu` are indeed perfectly correlated. Upon further inspection it does not seem to be a simulation error. It seems as if `tbf_iu` and `prodbf_iu` do the exact same thing. It would mean that the product of the unconstrained Bayes Factors is the same as the unconstrained Bayes Factor under the condition that the hypothesis holds for all groups $k$.

With this in the back of our mind, let us continue.
Now we subset for when true BF > 3 and describe the distribution for those conditional medians.
```{r, echo = F}
es_gt_hyp_val <- descriptives(dat[es == .2 ,lapply(.SD, median), .SDcols = varsout, by = varspred])
datatable(round(es_gt_hyp_val,5),options = list(dom = "t"))
```

```{r}
all(es_gt_hyp_val['sum>3',] == marginal_descs['sum>3',]) 
```

Even when the `data.table` is subsetted for when es > hyp_val no median BF gets above 3. Also, all cases where the median BF does exceed 3 is indeed only in the conditions when es > hyp_val, suggesting good algorithmic performance when in reality there is no effect. Again, the MADs are not too severe, except for the prodbf_ic, suggesting the prod_bf algorithm is less stable when BF > 3 and can calculate large estimates even when the true_es is just 0.1 point higher than the hypothesized value. Again, the unconstrained algorithms seem to generally be more conservative than the complementary. 

When true BF < 3
```{r, echo =F}
es_st_hyp_val <- descriptives(dat[es == .1 ,lapply(.SD, median), .SDcols = varsout, by = varspred])
datatable(round(es_st_hyp_val,5), options = list(dom = 't'))
```
Further supporting the idea of conservative algorithms is the finding is that the maximum median BFs in the cases where es = hyp_val all are $\approx 1$ and  all $75^{th}$ percentiles are slightly below one. The algorithms seem to favour the hypotheses that are not in the hypothesized direction, although very moderately. The previously noticed effect of the unconstrained vs the complement algorithms is not immediately visible.


Correlation between algorithms, first for the dt with median BFs than for entire dt.
```{r, echo = F}
cormedian <- cor(dat[,lapply(.SD, median), .SDcols = varsout, by = varspred][,..varsout])
datatable(round(cormedian,5), options = list(dom = 't'))
```
A first observation is that `allsig` is only correlated moderately with the `gpbf` algorithms and not at all with the others. Note that this correlation table consists of 72 values per algorithm. The other general trend is that the `gpbf` algorithms have the lowest correlation with `prodbf_ic` and `allsig` and the highest with each other. Finally, the `prodbf` algorithms correlate highly with `tbf` and fairly high with `gpbf`, but less with each other. The `tbf` algorithms do correlate highly together.

For the entire dt
```{r, echo = F}
corcomplete <- cor(dat[,..varsout])
datatable(round(corcomplete,5), options = list(dom = 't'))
```
We see roughly the same trends. The only real difference seems that all correlation with `prodbf_ic` have lowered quite a bit. This is likely due to the instable nature of the algorithm estimating very large estimates, while the other algorithms are more reserved.

Inspect the conditions where median prod_bf > 3. Also ordered by k and prodbc_ic values.
```{r, echo = F}
prodbf_gt_3 <- dat[,lapply(.SD, median), .SDcols = varsout, by = varspred][prodbf_ic > 3][order(k, prodbf_ic, decreasing = T)]
datatable(round(prodbf_gt_3,5), options = list(dom = 't'))
```
First of all, the conditions where the median BF exceeds three for `prodbf_ic`, are also the conditions where it exceeds three for all other algorithms, except `allsig.` The only condition where the median BF for `allsig` exceeded 3 was when $es = 0.2, errorsd = 0, n = 500, k = 1$, and $hyp\_val = 0.1$.
It is evident that the higher values for the median BF occur for higher values of $k$, with ten out of sixteen times $k = 10$ and the rest when $k = 3$. This makes sense as the `prod_bf` algorithms take the product of the Bayes Factor for all $k$ groups. The likelihood of BF_ic and BF_iu being bigger than 1 or a specific group increases as $es > hyp\_val$, resulting in big numbers when all BFs are multiplied. The finding that the other algorithms are more conservative than the `prodbf_ic` is likely because they correct for the number of groups. For the `gpbf` algorithms by raising the `prodbf` to the power $\frac{1}{k}$. The `tbf` algorithms correct for $k$ by calculating the BFs under the condition that the effect in all $k$ groups together is bigger than the hypothesized value, rather than for all $k$ groups separately. The `prodbf_iu` algorithm does not necessarily correct for $k$, but it is not restricted to testing only against the complement of the hypothesized direction. It rather tests agains all hypotheses, except the informative. In other words, how much more likely is the informative hypothesis than all other hypotheses, rather than only the complement of the informative hypothesis.

The other condition that seems to have some importance is $errorsd$, where most algorithms found an effect when $errorsd \neq 0.81$ and preferably when $errorsd = 0$. 

## Confusion matrix metrics
```{r, echo = F}
# obtain confusion matrix metrics with a dimension of 1 by alg*metrics (so 1x28 in this datatable)
lapply_at <- function(var, truees) {
  results <- sapply(var, function(var) {
    table(ordered(truees > .1, levels = c("FALSE", "TRUE")), ordered(var > 3, levels = c("FALSE", "TRUE")))
  })
  names(results) <- vapply(names(var), paste, c("TN", "FN", "FP", "TP"), sep = "_", 
                           FUN.VALUE = character(4),
                           USE.NAMES = FALSE)
  as.list(results)
}

# in long format gives a 4x7 matrix with 4 metrics and 7 algorithms
metrics <- function(datf){
  res <- datf[, sapply(.SD, function(var) {
    table(ordered(es > .1, levels = c("FALSE", "TRUE")), ordered(var > 3, levels = c("FALSE", "TRUE")))
  }), .SDcols = varsout]
  rownames(res) <- c("TN", "FN", "FP", "TP")
  return(res)
}

# obtain metrics per algorithm based on FP, TN, FN and TP() and collect in a matrix
get_stats <- function(res){ 
  apply(res, 2, function(x){
    attach(as.list(x))
    alpha = FP / (FP + TN) #False positive rate 
    beta = FN / (TP + FN) # False negative rate
    sensitivity = 1-beta
    specificity = 1-alpha
    correct = (TN + TP) /(FP + FN + TN + TP)
    pos_lr = sensitivity / (1 - specificity)
    neg_lr = (1 - sensitivity) / specificity
    c(correct = correct,
      False_positive_rate_alpha = alpha,
      False_negative_rate_beta = beta,
      sensitivity = sensitivity,
      specificity = specificity, 
      pos_lr = ifelse(is.na(pos_lr), 0, pos_lr),
      neg_lr = ifelse(is.na(neg_lr), 0, neg_lr))
  })
}

# create df from stats matrix for plotting purposes
plot_df <-function(stats){ 
  data.frame(Method = colnames(stats),
        correct = stats["correct", ],
        Sensitivity = stats["sensitivity", ],
        Specificity = stats["specificity", ],
        FPR = stats["False_positive_rate_alpha", ],
        FNR = stats["False_negative_rate_beta", ],
        Pos_lr = stats["pos_lr", ],
        Neg_lr = stats["neg_lr", ])
}

# all in one function. Create plot df for a subset of the data table.
create_df_plot <- function(datf){
  metrics_long <- metrics(datf)
  stats <- get_stats(metrics_long)
  df_plot <- plot_df(stats)
  return(df_plot)
}
```

Marginal statistics
```{r, echo = F, message=F}
df_plot <- create_df_plot(dat)
marginal_metrics <- rbind(df_plot[,2:ncol(df_plot)], 
      mean = apply(df_plot[,2:ncol(df_plot)], 2 ,mean))
datatable(round(marginal_metrics, 5), options = list(dom = 't'))
```
Firstly, on average, the algorithms were correct in 58\% of the cases. The `prodbf_ic` did best with 68\% identified correctly, followed by 62\% of the tbf_ic.The gbpf algorithms do overall worst with $\approx 50\%$ being identified correctly. Also interesting, all _ic algorithms do better than their _iu counterparts.
Secondly, all measures also suggest somewhat conservative algorithms, with a mild exception for prodbf_ic. The mean sensitivity of the algorithms is 0.17, meaning that on average, out of all cases where BF > 3, only 17\% is identified correctly. Interestingly, the gpbf algorithms are especially conservative. The tbf algorithms and prodbf_iu do slightly better in this regard. An obvious outlier is the prodbf_ic algorithm with 44\% correctly identified when BF > 3.
Where the algorithms make up is in the specificity, with a mean specificty of 0.98. This implies that out of all cases where BF < 3, 98\% is identified correctly on average. The prodbf_ic performs only slightly worse than the other algorithms, although the differences are not too severe.

<!--add part about LR+ and LR- -->


<!-- below is in the make...
```{r, eval = F}
# conditional on n
ndf <- rbind(create_df_plot(dat[n==20]),
           create_df_plot(dat[n==80]),
           create_df_plot(dat[n==200]),
           create_df_plot(dat[n==500]))
ndf$cond <- c(rep("n = 20", 7), rep("n = 80", 7), rep("n = 200", 7), rep("n = 500", 7))

ggplot(ndf, aes(x = Sensitivity, y = Specificity, colour = Method, shape = cond)) +
  geom_point() +
  scale_x_continuous(limits = c(0,1))+
  scale_y_continuous(limits = c(0.75,1)) +
  theme_bw()


# conditional on errorsd
errorsddf <- rbind(create_df_plot(dat[errorsd == 0]),
             create_df_plot(dat[errorsd == 0.5]),
             create_df_plot(dat[errorsd == 0.81]))
errorsddf$cond <- c(rep("errorsd = 0", 7), rep("errorsd = 0.5", 7), rep("errorsd = 0.81", 7))

ggplot(errorsddf, aes(x = Sensitivity, y = Specificity, colour = Method, shape = cond)) +
  geom_point() +
  scale_x_continuous(limits = c(0,1))+
  scale_y_continuous(limits = c(0,1)) +
  theme_bw()

```
-->
